\relax 
\providecommand\hyper@newdestlabel[2]{}
\bibstyle{abbrvnat}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{wang2018pfc}
\citation{wang2018pfc}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction to Meta-Reinforcement Learning}{1}{section.1}\protected@file@percent }
\citation{wang2018pfc,botvinick2019408}
\citation{mazzoni1991}
\@writefile{toc}{\contentsline {section}{\numberline {2}(Multi-Agent) Meta-Learning RL to find Optimal Learning Rules}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{PettingZoo and others...}{2}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Comparison to Known Learning Rules}{2}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces In both figures, we have a neural network as the lower loop. For any given timestep $t$, we have a policy $\pi (\theta _t)$ which governs the weight updates as the neural network performs \texttt  {epochs = S} forward passes. There are a total of $\mathcal  {T}$ different random initializations of weights at each timestep, each running for $S$ epochs. At each epoch, the network emits a reward corresponding to the negative loss, which the agent will use to improve its policy for timestep $t+1$ to create $\pi (\theta _{t+1})$. On the left, we have a Multi-Agent Meta-RL system, where each agent is in control of a set of weights and can only see the previous layer's activations. Each agent receives the same total reward. On the right, we have a single agent that does all of the work of the agents in the multi-agent regime, thereby having complete knowledge of all weights. }}{3}{figure.1}\protected@file@percent }
\newlabel{fig:meta-rl}{{1}{3}{In both figures, we have a neural network as the lower loop. For any given timestep $t$, we have a policy $\pi (\theta _t)$ which governs the weight updates as the neural network performs \texttt {epochs = S} forward passes. There are a total of $\mathcal {T}$ different random initializations of weights at each timestep, each running for $S$ epochs. At each epoch, the network emits a reward corresponding to the negative loss, which the agent will use to improve its policy for timestep $t+1$ to create $\pi (\theta _{t+1})$. On the left, we have a Multi-Agent Meta-RL system, where each agent is in control of a set of weights and can only see the previous layer's activations. Each agent receives the same total reward. On the right, we have a single agent that does all of the work of the agents in the multi-agent regime, thereby having complete knowledge of all weights}{figure.1}{}}
\newlabel{forward}{{1}{3}{Comparison to Known Learning Rules}{equation.3.1}{}}
\newlabel{mseloss}{{2}{3}{Comparison to Known Learning Rules}{equation.3.2}{}}
\newlabel{crossentropy}{{3}{3}{Comparison to Known Learning Rules}{equation.3.3}{}}
\citation{hiratani2022on}
\citation{hiratani2022on}
\@writefile{toc}{\contentsline {paragraph}{Training on Student-Teacher (Fig. \ref  {fig:teachstudent})}{4}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training on MNIST (Fig. \ref  {fig:mnist})}{4}{section*.3}\protected@file@percent }
\bibdata{sample}
\bibcite{botvinick2019408}{{1}{2019}{{Botvinick et~al.}}{{Botvinick, Ritter, Wang, Kurth-Nelson, Blundell, and Hassabis}}}
\bibcite{hiratani2022on}{{2}{2022}{{Hiratani et~al.}}{{Hiratani, Mehta, Lillicrap, and Latham}}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Discuss the results from teacher-student here }}{5}{figure.2}\protected@file@percent }
\newlabel{fig:teachstudent}{{2}{5}{Discuss the results from teacher-student here}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Meta-Reinforcement Learning in the Brain}{5}{section.4}\protected@file@percent }
\bibcite{mazzoni1991}{{3}{1991}{{P et~al.}}{{P, RA, and MI}}}
\bibcite{wang2018pfc}{{4}{2018}{{Wang et~al.}}{{Wang, Kurth-Nelson, Kumaran, Tirumala, Soyer, Leibo, Hassabis, and Botvinick}}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Discuss the results from mnist here }}{6}{figure.3}\protected@file@percent }
\newlabel{fig:mnist}{{3}{6}{Discuss the results from mnist here}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Discuss the results from mnist here }}{7}{figure.4}\protected@file@percent }
\newlabel{fig:mnistacc}{{4}{7}{Discuss the results from mnist here}{figure.4}{}}
