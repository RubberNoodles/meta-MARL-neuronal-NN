\relax 
\providecommand\hyper@newdestlabel[2]{}
\bibstyle{abbrvnat}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{wang2018pfc}
\citation{wang2018pfc}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction to Meta-Reinforcement Learning}{1}{section.1}\protected@file@percent }
\citation{wang2018pfc,botvinick2019408}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces In both figures, we have a neural network as the lower loop. For any given timestep $t$, we have a policy $\pi (\theta _t)$ which governs the weight updates as the neural network performs \texttt  {epochs = S} forward passes. There are a total of $\mathcal  {T}$ different random initializations of weights at each timestep, each running for $S$ epochs. At each epoch, the network emits a reward corresponding to the negative loss, which the agent will use to improve its policy for timestep $t+1$ to create $\pi (\theta _{t+1})$. On the left, we have a Multi-Agent Meta-RL system, where each agent is in control of a set of weights and can only see the previous layer's activations. Each agent receives the same total reward. On the right, we have a single agent that does all of the work of the agents in the multi-agent regime, thereby having complete knowledge of all weights. }}{2}{figure.1}\protected@file@percent }
\newlabel{fig:meta-rl}{{1}{2}{In both figures, we have a neural network as the lower loop. For any given timestep $t$, we have a policy $\pi (\theta _t)$ which governs the weight updates as the neural network performs \texttt {epochs = S} forward passes. There are a total of $\mathcal {T}$ different random initializations of weights at each timestep, each running for $S$ epochs. At each epoch, the network emits a reward corresponding to the negative loss, which the agent will use to improve its policy for timestep $t+1$ to create $\pi (\theta _{t+1})$. On the left, we have a Multi-Agent Meta-RL system, where each agent is in control of a set of weights and can only see the previous layer's activations. Each agent receives the same total reward. On the right, we have a single agent that does all of the work of the agents in the multi-agent regime, thereby having complete knowledge of all weights}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}(Multi-Agent) Meta-Learning RL to find Optimal Learning Rules}{2}{section.2}\protected@file@percent }
\citation{openaigym,sb3}
\citation{tianshou,pettingzoo}
\citation{mazzoni1991}
\@writefile{toc}{\contentsline {paragraph}{Code Implementation}{3}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Comparison to Known Learning Rules}{3}{section.3}\protected@file@percent }
\newlabel{forward}{{1}{3}{Comparison to Known Learning Rules}{equation.3.1}{}}
\newlabel{mseloss}{{2}{3}{Comparison to Known Learning Rules}{equation.3.2}{}}
\newlabel{crossentropy}{{3}{3}{Comparison to Known Learning Rules}{equation.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces We find that the policy that the meta-RL system finds is to instantly jump to a certain set of weights, and then perform no further updates. The learning rates were chosen to optimize the loss, in the range of $\eta =1.0$ to $0.1$. $\sigma = \frac  {1}{L_y L_h}$. }}{4}{figure.2}\protected@file@percent }
\newlabel{fig:teachstudent}{{2}{4}{We find that the policy that the meta-RL system finds is to instantly jump to a certain set of weights, and then perform no further updates. The learning rates were chosen to optimize the loss, in the range of $\eta =1.0$ to $0.1$. $\sigma = \frac {1}{L_y L_h}$}{figure.2}{}}
\citation{hiratani2022on}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Again, we see the Meta-RL NN learn its weights instantly, and as a result have a constant loss for both training and testing. NP is set to $\eta = 10^{-4}$, which is unstable. NP will converge for $\eta \ll 10^{-6}$, but the number of epochs needed to make any reasonable progress was too high to be computationally feasible. $\sigma = \frac  {1}{L_y L_h}$.}}{5}{figure.3}\protected@file@percent }
\newlabel{fig:mnist}{{3}{5}{Again, we see the Meta-RL NN learn its weights instantly, and as a result have a constant loss for both training and testing. NP is set to $\eta = 10^{-4}$, which is unstable. NP will converge for $\eta \ll 10^{-6}$, but the number of epochs needed to make any reasonable progress was too high to be computationally feasible. $\sigma = \frac {1}{L_y L_h}$}{figure.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Training on Student-Teacher (Fig. \ref  {fig:teachstudent})}{5}{section*.2}\protected@file@percent }
\citation{hiratani2022on}
\citation{wang2018pfc}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Test accuracy for Meta-RL is constant due to the policy network choosing the optimal weights during training instantly. NP is under performing due to being excessively noisy. }}{6}{figure.4}\protected@file@percent }
\newlabel{fig:mnistacc}{{4}{6}{Test accuracy for Meta-RL is constant due to the policy network choosing the optimal weights during training instantly. NP is under performing due to being excessively noisy}{figure.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Training on MNIST (Fig. \ref  {fig:mnist})}{6}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion and Further Steps}{6}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Meta-Reinforcement Learning in the Brain}{6}{section*.4}\protected@file@percent }
\citation{wang2018pfc}
\bibdata{sample}
\bibcite{botvinick2019408}{{1}{2019}{{Botvinick et~al.}}{{Botvinick, Ritter, Wang, Kurth-Nelson, Blundell, and Hassabis}}}
\bibcite{openaigym}{{2}{2016}{{Brockman et~al.}}{{Brockman, Cheung, Pettersson, Schneider, Schulman, Tang, and Zaremba}}}
\bibcite{hiratani2022on}{{3}{2022}{{Hiratani et~al.}}{{Hiratani, Mehta, Lillicrap, and Latham}}}
\bibcite{mazzoni1991}{{4}{1991}{{P et~al.}}{{P, RA, and MI}}}
\bibcite{sb3}{{5}{2021}{{Raffin et~al.}}{{Raffin, Hill, Gleave, Kanervisto, Ernestus, and Dormann}}}
\bibcite{pettingzoo}{{6}{2020}{{Terry et~al.}}{{Terry, Black, Grammel, Jayakumar, Hari, Sulivan, Santos, Perez, Horsch, Dieffendahl, Williams, Lokesh, Sullivan, and Ravi}}}
\bibcite{wang2018pfc}{{7}{2018}{{Wang et~al.}}{{Wang, Kurth-Nelson, Kumaran, Tirumala, Soyer, Leibo, Hassabis, and Botvinick}}}
\bibcite{tianshou}{{8}{2022}{{Weng et~al.}}{{Weng, Chen, Yan, You, Duburcq, Zhang, Su, Su, and Zhu}}}
\@writefile{toc}{\contentsline {paragraph}{Computation Limits and Further Steps}{7}{section*.5}\protected@file@percent }
